{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\dd-resource\\slides\\homework\\ML\\HW1\n"
     ]
    }
   ],
   "source": [
    "cd G:\\dd-resource\\slides\\homework\\ML\\HW1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is an implemetation of gradient_descent with RANSAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this theta0 is:  4.90627073063 and this theta1 is:  -0.134975812309 and this error is:  34.5656467306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: RuntimeWarning: overflow encountered in double_scalars\n",
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: RuntimeWarning: overflow encountered in double_scalars\n",
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:95: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better theta0 is:  5.82505416164 and better theta1 is:  -0.333636425112 and better error is:  1.95343952232\n",
      "linear square error is:  1.75992360748\n",
      "robust error is:  0.208449999343\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhuAGgMAQ3AAQGIIbAAJDcANAYP4/JYxroUdsjjoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2af791345c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have done!!!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pylab\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "def gradient_descent(alpha, x, y, accurancy, max_iter):\n",
    "    converged = False\n",
    "    iter = 0\n",
    "    m = x.shape[0]  # number of samples\n",
    "\n",
    "    # initial theta\n",
    "    # random.seed(3)\n",
    "    t0 = np.random.random() #   5\n",
    "    t1 = np.random.random() #   -0.3\n",
    "\n",
    "\n",
    "    # total error, J(theta)\n",
    "    J = sum([(t0 + t1 * x[i] - y[i]) ** 2 for i in range(m)])\n",
    "\n",
    "    # Iterate Loop\n",
    "    while not converged:\n",
    "        # for each training sample, compute the gradient (d/d_theta j(theta))\n",
    "        grad0 = 1.0 / m * sum([(t0 + t1 * x[i] - y[i]) for i in range(m)])\n",
    "        grad1 = 1.0 / m * sum([(t0 + t1 * x[i] - y[i]) * x[i] for i in range(m)])\n",
    "\n",
    "        # update the theta_temp\n",
    "        temp0 = t0 - alpha * grad0\n",
    "        temp1 = t1 - alpha * grad1\n",
    "\n",
    "        # update theta\n",
    "        t0 = temp0\n",
    "        t1 = temp1\n",
    "\n",
    "        # mean squared error\n",
    "        e = sum([(t0 + t1 * x[i] - y[i]) ** 2 for i in range(m)])\n",
    "\n",
    "        if abs(J - e) <= accurancy:\n",
    "            # print(\"Converged, iterations: \", iter, \"!!!\")\n",
    "            converged = True\n",
    "\n",
    "        J = e  # update error\n",
    "        iter += 1  # update iter\n",
    "\n",
    "        if iter == max_iter:\n",
    "            # print(\"Max interactions exceeded!\")\n",
    "            converged = True\n",
    "\n",
    "    return t0, t1, J\n",
    "\n",
    "\n",
    "def read_file(num):\n",
    "    def read_data(edge):\n",
    "        x, y = edge.split()\n",
    "        return int(x) / 10, int(y) / 100\n",
    "\n",
    "\n",
    "    if(num==1):\n",
    "        f = open(\"5-trainingdata.txt\")\n",
    "    else:\n",
    "        f = open(\"5-testdata.txt\")\n",
    "    string = f.readline()\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    if(num==1):\n",
    "        edges = [read_data(f.readline()) for _ in range(32)]\n",
    "    else:\n",
    "        edges = [read_data(f.readline()) for _ in range(6)]\n",
    "    for x, y in edges:\n",
    "        x_points.append(x)\n",
    "        y_points.append(y)\n",
    "    f.close()\n",
    "    return edges,np.array(x_points), np.asarray(y_points)\n",
    "\n",
    "\n",
    "def ransac_robust(max_iter,edges,alpha,accurancy,threhold,dd):\n",
    "    best_theta0=0\n",
    "    best_theta1=0\n",
    "    best_error=float('inf')\n",
    "    iteration=0\n",
    "    maybe_inliers=[]\n",
    "    maybe_model=[]\n",
    "    best_inliers=[]\n",
    "    best_inliers_values=[]\n",
    "    while (iteration<max_iter):\n",
    "        maybe_data=random.sample(edges,2)\n",
    "        for x, y  in maybe_data:\n",
    "            maybe_inliers.append(x)\n",
    "            maybe_model.append(y)\n",
    "        theta0, theta1, this_error= gradient_descent(alpha, np.asarray(maybe_inliers), np.asarray(maybe_model), accurancy, max_iter=10000)\n",
    "        consensus_set=maybe_inliers\n",
    "        consensus_set_value = maybe_model\n",
    "        maybe_outliers=[edge for edge in edges if edge not in maybe_inliers]\n",
    "        for out_edge in maybe_outliers:\n",
    "            loss=(theta0 + theta1 * out_edge[0] - out_edge[1])**2\n",
    "            if (loss<threhold):\n",
    "                consensus_set.append(out_edge[0])\n",
    "                consensus_set_value.append(out_edge[1])\n",
    "            if(len(maybe_inliers)>dd):\n",
    "                break\n",
    "            better_theta0, better_theta1, best_error = gradient_descent(alpha, np.array(consensus_set), np.array(consensus_set_value), accurancy,\n",
    "                                          max_iter=10000)\n",
    "            if(this_error<best_error):\n",
    "                best_inliers=consensus_set\n",
    "                best_inliers_values=consensus_set_value\n",
    "                best_error=this_error\n",
    "                best_theta0=better_theta0\n",
    "                best_theta1=better_theta1\n",
    "        iteration=iteration+1\n",
    "    return best_theta0,best_theta1,best_error,best_inliers,best_inliers_values\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    edges,x_points, y_points = read_file(1)\n",
    "    alpha = 0.01  # learning rate\n",
    "    accurancy = 1e-10  # convergence criteria\n",
    "    threhold=1e-10  # threhold: 决定数据是否适应于模型的阈值\n",
    "    dd=10   # dd: 适用于数据集的数据数目\n",
    "    theta0, theta1, this_error = gradient_descent(alpha, x_points, y_points, accurancy, max_iter=10000)\n",
    "    print(\"this theta0 is: \", theta0, \"and this theta1 is: \", theta1, \"and this error is: \", this_error)\n",
    "    #   theta0_initial=5~6, theta1_initial=-0.3~-0.4\n",
    "    best_theta0, best_theta1, best_error, best_inliers, best_inliers_values = ransac_robust(50,edges,0.05,accurancy,threhold,dd)\n",
    "    print(\"better theta0 is: \", best_theta0, \"and better theta1 is: \", best_theta1, \"and better error is: \", best_error)\n",
    "    arr = np.asarray(best_inliers)\n",
    "    for i in range(x_points.shape[0]):\n",
    "        y_predict1 = theta0 + theta1 * x_points\n",
    "\n",
    "\n",
    "    for i in range(arr.shape[0]):\n",
    "        y_predict2= best_theta0 + best_theta1 * arr\n",
    "\n",
    "    test_edges,test_x_points, test_y_points=read_file(0)\n",
    "    test_m=test_x_points.shape[0]\n",
    "    test_error1= sum([(theta0 + theta1 * test_x_points[i] - test_y_points[i]) ** 2 for i in range(test_m)])\n",
    "    test_error2= sum([(best_theta0 + best_theta1 * test_x_points[i] - test_y_points[i]) ** 2 for i in range(test_m)])\n",
    "    print(\"linear square error is: \", test_error1)\n",
    "    print(\"robust error is: \", test_error2)\n",
    "\n",
    "    pylab.plot(x_points, y_points, 'o')\n",
    "    pylab.plot(x_points, y_predict1, 'k-',color='red')\n",
    "    pylab.plot(arr, y_predict2, 'k-',color='blue')\n",
    "    pylab.show()\n",
    "    print(\"Have done!!!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
