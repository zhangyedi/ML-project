{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\dd-resource\\slides\\homework\\ML\\HW1\n"
     ]
    }
   ],
   "source": [
    "cd G:\\dd-resource\\slides\\homework\\ML\\HW1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is an implemetation of gradient_descent with Huber Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def huber_loss(x,d):\n",
    "    x=np.abs(x)\n",
    "    return (x<=d)*x**2/2+(x>d)*d*(x-d/2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent2(alpha, x, y, d,accurancy, max_iter=10000):\n",
    "    converged = False\n",
    "    iter = 0\n",
    "    m = x.shape[0]  # number of samples\n",
    "\n",
    "    # initial theta\n",
    "    # np.random.seed(1)\n",
    "    t0 = np.random.random()\n",
    "    t1 = np.random.random()\n",
    "\n",
    "    # total error, J(theta)\n",
    "    # J = sum([(t0 + t1 * x[i] - y[i]) ** 2 for i in range(m)])\n",
    "    J = sum([huber_loss((t0 + t1 * x[i] - y[i]), d) for i in range(m)])\n",
    "\n",
    "    # Iterate Loop\n",
    "    while not converged:\n",
    "        # for each training sample, compute the gradient (d/d_theta j(theta))\n",
    "        # grad0 = 1.0 / m * sum([(t0 + t1 * x[i] - y[i]) for i in range(m)])\n",
    "        # grad1 = 1.0 / m * sum([(t0 + t1 * x[i] - y[i]) * x[i] for i in range(m)])\n",
    "        grad0=0\n",
    "        for i in range(m):\n",
    "            uni_err=t0 + t1 * x[i] - y[i]\n",
    "            if(np.abs(uni_err)<=d):\n",
    "                grad0=grad0+ uni_err/2\n",
    "            elif(uni_err>d):\n",
    "                grad0=grad0+d/2\n",
    "            elif(uni_err<-d):\n",
    "                grad0=grad0-d/2\n",
    "        grad0=grad0/m\n",
    "\n",
    "        grad1 = 0\n",
    "\n",
    "        for i in range(m):\n",
    "            uni_err = t0 + t1 * x[i] - y[i]\n",
    "            if (np.abs(uni_err) <= d):\n",
    "                grad1 = grad1 + uni_err * x[i] / 2\n",
    "            elif (uni_err > d):\n",
    "                grad1 = grad1 + d * x[i] / 2\n",
    "            elif (uni_err < -d):\n",
    "                grad1 = grad1 - d * x[i] / 2\n",
    "\n",
    "        grad1 = grad1 / m\n",
    "\n",
    "        # update the theta_temp\n",
    "        temp0 = t0 - alpha * grad0\n",
    "        temp1 = t1 - alpha * grad1\n",
    "\n",
    "        # update theta\n",
    "        t0 = temp0\n",
    "        t1 = temp1\n",
    "\n",
    "        # mean squared error\n",
    "        #e = sum([(t0 + t1 * x[i] - y[i]) ** 2 for i in range(m)])\n",
    "        e = sum([huber_loss((t0 + t1 * x[i] - y[i]), d) for i in range(m)])\n",
    "\n",
    "        if abs(J - e) <= accurancy:\n",
    "            print(\"Converged, iterations: \", iter, \"!!!\")\n",
    "            converged = True\n",
    "\n",
    "        J = e  # update error\n",
    "        iter += 1  # update iter\n",
    "\n",
    "        if iter == max_iter:\n",
    "            print(\"Max interactions exceeded!\")\n",
    "            converged = True\n",
    "\n",
    "    return t0, t1, J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent1(alpha, x, y, accurancy, max_iter=10000):\n",
    "    converged = False\n",
    "    iter = 0\n",
    "    m = x.shape[0]  # number of samples\n",
    "\n",
    "    # initial theta\n",
    "    # np.random.seed(1)\n",
    "    t0 = np.random.random()\n",
    "    t1 = np.random.random()\n",
    "\n",
    "    # total error, J(theta)\n",
    "\n",
    "    J = sum([(t0 + t1 * x[i] - y[i]) ** 2 for i in range(m)])\n",
    "    # J = sum([huber_loss((t0 + t1 * x[i] - y[i]),d) for i in range (m)])\n",
    "    # Iterate Loop\n",
    "    while not converged:\n",
    "        # for each training sample, compute the gradient (d/d_theta j(theta))\n",
    "        grad0 = 1.0 / m * sum([(t0 + t1 * x[i] - y[i]) for i in range(m)])\n",
    "        grad1 = 1.0 / m * sum([(t0 + t1 * x[i] - y[i]) * x[i] for i in range(m)])\n",
    "\n",
    "        # update the theta_temp\n",
    "        temp0 = t0 - alpha * grad0\n",
    "        temp1 = t1 - alpha * grad1\n",
    "\n",
    "        # update theta\n",
    "        t0 = temp0\n",
    "        t1 = temp1\n",
    "\n",
    "        # mean squared error\n",
    "        e = sum([(t0 + t1 * x[i] - y[i]) ** 2 for i in range(m)])\n",
    "        # e = sum([huber_loss((t0 + t1 * x[i] - y[i]), d)for i in range(m)] )\n",
    "\n",
    "        if abs(J - e) <= accurancy:\n",
    "            print(\"Converged, iterations: \", iter, \"!!!\")\n",
    "            converged = True\n",
    "\n",
    "        J = e  # update error\n",
    "        iter += 1  # update iter\n",
    "\n",
    "        if iter == max_iter:\n",
    "            print(\"Max interactions exceeded!\")\n",
    "            converged = True\n",
    "\n",
    "    return t0, t1, J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best_d_value():\n",
    "    edges,x_points, y_points = read_file(1)\n",
    "    alpha = 0.01  # learning rate\n",
    "    accurancy = 1e-10  # convergence criteria\n",
    "    for d in (0.2,0.3,0.4,0.5,0.7,1,3,5):\n",
    "        best_theta0_d, best_theta1_d, error_d = gradient_descent2(alpha, x_points, y_points, d, accurancy,\n",
    "                                                                 max_iter=10000)\n",
    "        pylab.plot(d, error_d,'o-',color='green')\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(num):\n",
    "    def read_data(edge):\n",
    "        x, y = edge.split()\n",
    "        return int(x) / 10, int(y) / 100\n",
    "\n",
    "\n",
    "    if(num==1):\n",
    "        f = open(\"5-trainingdata.txt\")\n",
    "    else:\n",
    "        f = open(\"5-testdata.txt\")\n",
    "    string = f.readline()\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    if(num==1):\n",
    "        edges = [read_data(f.readline()) for _ in range(32)]\n",
    "    else:\n",
    "        edges = [read_data(f.readline()) for _ in range(6)]\n",
    "    for x, y in edges:\n",
    "        x_points.append(x)\n",
    "        y_points.append(y)\n",
    "    f.close()\n",
    "    return edges,np.array(x_points), np.asarray(y_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged, iterations:  6935 !!!\n",
      "this theta0 is:  4.90627079504 and this theta1 is:  -0.134975822973 and this error is:  34.5656467306\n",
      "Max interactions exceeded!\n",
      "better theta0 is:  5.31575448533 and better theta1 is:  -0.219398743092 and better error is:  6.87174522796\n",
      "linear square error is:  1.75992346086\n",
      "huber error is:  0.871712651868\n",
      "Have done!!!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwCi4RUQCo+AWEQnM/wdCtQp9w64VTwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22050c7e8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    edges,x_points, y_points = read_file(1)\n",
    "    alpha = 0.01  # learning rate\n",
    "    accurancy = 1e-10  # convergence criteria\n",
    "    d=0.5\n",
    "\n",
    "    theta0, theta1, this_error = gradient_descent1(alpha, x_points, y_points, accurancy, max_iter=10000)\n",
    "    print(\"this theta0 is: \", theta0, \"and this theta1 is: \", theta1, \"and this error is: \", this_error)\n",
    "    best_theta0, best_theta1, best_error = gradient_descent2(alpha, x_points, y_points, d, accurancy, max_iter=10000)\n",
    "    print(\"better theta0 is: \", best_theta0, \"and better theta1 is: \", best_theta1, \"and better error is: \", best_error)\n",
    "\n",
    "\n",
    "    for i in range(x_points.shape[0]):\n",
    "        y_predict1 = theta0 + theta1 * x_points\n",
    "\n",
    "    for i in range(x_points.shape[0]):\n",
    "        y_predict2= best_theta0 + best_theta1 * x_points\n",
    "\n",
    "\n",
    "\n",
    "    test_edges, test_x_points, test_y_points = read_file(0)\n",
    "    test_m = test_x_points.shape[0]\n",
    "    test_error1 = sum([(theta0 + theta1 * test_x_points[i] - test_y_points[i]) ** 2 for i in range(test_m)])\n",
    "    # test_error2 = sum([ huber_loss((best_theta0 + best_theta1 * test_x_points[i] - test_y_points[i]), d) for i in range(test_m)])\n",
    "    test_error2 = sum([(best_theta0 + best_theta1 * test_x_points[i] - test_y_points[i]) ** 2 for i in range(test_m)])\n",
    "    print(\"linear square error is: \", test_error1)\n",
    "    print(\"huber error is: \", test_error2)\n",
    "    print(\"Have done!!!\")\n",
    "\n",
    "    pylab.plot(x_points, y_points, 'o')\n",
    "    pylab.plot(x_points, y_predict1, 'k-', color='red')\n",
    "    pylab.plot(x_points, y_predict2, 'k-', color='blue')\n",
    "    pylab.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
